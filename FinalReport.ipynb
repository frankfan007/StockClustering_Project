{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'pandas' has no attribute 'core'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-7c5a9bb5cbd1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnetworkx\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/cheungcecilia/anaconda/lib/python3.5/site-packages/pandas/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig_init\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/cheungcecilia/anaconda/lib/python3.5/site-packages/pandas/core/api.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0misnull\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnotnull\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcategorical\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCategorical\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGrouper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mset_eng_float_format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m from pandas.core.index import (Index, CategoricalIndex, Int64Index,\n",
      "\u001b[0;32m/Users/cheungcecilia/anaconda/lib/python3.5/site-packages/pandas/core/groupby.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m                               DataError, SpecificationError)\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcategorical\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCategorical\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframe\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneric\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNDFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m from pandas.core.index import (Index, MultiIndex, CategoricalIndex,\n",
      "\u001b[0;32m/Users/cheungcecilia/anaconda/lib/python3.5/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m                                    \u001b[0mcreate_block_manager_from_arrays\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m                                    create_block_manager_from_blocks)\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseries\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcategorical\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCategorical\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomputation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpressions\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexpressions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/cheungcecilia/anaconda/lib/python3.5/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minternals\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSingleBlockManager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcategorical\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCategorical\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCategoricalAccessor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrings\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstrings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m from pandas.tseries.common import (maybe_to_datetimelike,\n\u001b[1;32m     36\u001b[0m                                    CombinedDatetimelikeProperties)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'pandas' has no attribute 'core'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab\n",
    "%matplotlib inline\n",
    "from IPython.display import display, HTML\n",
    "pylab.rcParams['figure.figsize'] = (10, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "HTML('''<script>\n",
    "code_show=true; \n",
    "function code_toggle() {\n",
    " if (code_show){\n",
    " $('div.input').hide();\n",
    " } else {\n",
    " $('div.input').show();\n",
    " }\n",
    " code_show = !code_show\n",
    "} \n",
    "$( document ).ready(code_toggle);\n",
    "</script>\n",
    "The raw code for this IPython notebook is by default hidden for easier reading.\n",
    "To toggle on/off the raw code, click <a href=\"javascript:code_toggle()\">here</a>.''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stock Market Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly,we will import our data, which consists of the following two *.csv* files:\n",
    "\n",
    "* **SP_500_firms.csv:** This fi\u001c",
    "le contains all \u001c",
    "firms currently included in the S&P 500 index.\n",
    "* **SP_500_close_2015.csv:** This fi\u001c",
    "le contains daily stock price data of the fi\u001c",
    "rms listed in the previous fi\u001c",
    "le for 2015 (without some \u001c",
    "firms for which data was not available for the entire year)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After importing the data, we inspect the first observations for each dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load companies information\n",
    "firms = pd.read_csv('SP_500_firms.csv', index_col = 0)\n",
    "# Load companies stock prices\n",
    "stockPrices = pd.read_csv('SP_500_close_2015.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "firms.iloc[:5, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stockPrices.iloc[:5, :5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The main part "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Stock returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Daily returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to analyse how similar those companies perform, we will look at their daily stock price movements. We will assume that companies that have similar stock price movements from day to day, perform similarly. To calculate the daily returns for all stocks in the data we define function `stockReturns` which takes as input the dataframe of the stock prices and returns a dataframe with the daily returns of those stocks. The daily returns are calculated based on the following formula:\n",
    "\n",
    "$$x_t = \\frac{p_t - p_t-1}{p_t-1}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def stockReturns(priceDF):\n",
    "    \n",
    "    compTickers = priceDF.columns[0: ]    \n",
    "    priceMat = priceDF.loc[ : , compTickers].as_matrix()    \n",
    "    diffMat = (priceMat[1: ] - priceMat[ :-1]) / priceMat[ :-1]\n",
    "    \n",
    "    return pd.DataFrame(data = diffMat, index = priceDF.index[1: ], \\\n",
    "                        columns = compTickers)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dailyReturns = stockReturns(stockPrices)\n",
    "dailyReturns.iloc[:5, :5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The daily returns of the stocks can be shown in the following figure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dailyReturns.plot(legend = False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will find which companies experienced the **maximum** and **minimum** daily returns,  and what potential evidence there may be for those extreme returns.\n",
    "\n",
    "To do so, we will marge the daily returns of all companies in one column and then sort this column in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dailyReturns['Date'] = dailyReturns.index\n",
    "# Melt data frame so each row is one price change observation\n",
    "dailyReturnsMelted = pd.melt(dailyReturns, id_vars = \"Date\")\n",
    "dailyReturnsMelted = dailyReturnsMelted.rename(columns = {\n",
    "    'variable':'Symbol',\n",
    "    'value':'Price Change'\n",
    "})\n",
    "# Sort melted dataframe in descending order of price change\n",
    "dailyReturnsSorted = dailyReturnsMelted.sort_values(by = 'Price Change', \\\n",
    "                                                    ascending = False)\n",
    "# Merge on firm data with the symbol as the key\n",
    "dailyReturnsSorted = dailyReturnsSorted.merge(firms, left_on = 'Symbol', \\\n",
    "                                              right_index = True, how = 'left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **maximum** daily returns for 2015 were the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dailyReturnsSorted.iloc[:10, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Potential evidence for some of the **maximum** daily returns presented above can be found on the following links:\n",
    "\n",
    "* Trip advisor: http://fortune.com/2015/10/14/tripadvisor-stock-gain-priceline-deal/\n",
    "* Williams: http://www.forbes.com/sites/antoinegara/2015/06/22/pipeline-giant-williams-rejects-64-a-share-takeover-bid-from-energy-transfer/#318056c339f8\n",
    "* Harman: http://investor.harman.com/releasedetail.cfm?releaseid=890984 and http://investor.harman.com/releasedetail.cfm?releaseid=893546 \n",
    "* Qorvo: http://www.qorvo.com/news/2015/qorvo-announces-proposed-1-billion-senior-notes-offering and http://www.bizjournals.com/triad/news/2015/11/06/qorvo-revenue-rises-in-latest-quarter.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **minimum** daily returns for 2015 were the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dailyReturnsSorted.iloc[-10:, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Potential evidence for some of the **minimum** daily returns presented above can be found on the following links:\n",
    "\n",
    "* Akami: http://www.fool.com/investing/general/2015/10/28/why-akamai-technologies-inc-fell-hard-on-wednesday.aspx\n",
    "* Millinckrodt: http://www.bloomberg.com/news/articles/2015-11-09/mallinckrodt-slumps-on-scrutiny-from-valeant-foe-citron-research\n",
    "* NRG Energy: http://247wallst.com/infrastructure/2015/12/04/nrg-continues-to-fall-as-ceo-steps-down/\n",
    "* Micron: http://marketrealist.com/2015/06/microns-share-price-fall-19-june-26/\n",
    "* Yum: http://www.reuters.com/article/us-yum-brands-china-idUSKCN0S11SZ20151007\n",
    "* Michael Kors: http://money.cnn.com/2015/05/27/investing/michael-kors-earnings-stock-drop/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Yearly returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will look at the overall performance of the S&P 500 companies over the whole year. Same as before, we create the function `yearlyStockReturns` which takes as input the dataframe of the stock prices and returns a dataframe with the yearly returns of those stocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def yearlyStockReturns(priceDF):\n",
    "    priceMatrix = priceDF.as_matrix()\n",
    "    # Calculate the yearly returns:\n",
    "    # (final price - start price) / start price\n",
    "    TotalPriceChangeMatrix = (priceMatrix[-1: ] - priceMatrix[ :1]) \\\n",
    "                              / priceMatrix[ :1]\n",
    "    # Convert the result to a dataframe \n",
    "    # with the correct index and column names\n",
    "    TotalPriceChangeDF = pd.DataFrame(TotalPriceChangeMatrix, \\\n",
    "                                      columns = priceDF.columns)\n",
    "    # Transpose dataframe\n",
    "    TotalPriceChangeDFtransposed = TotalPriceChangeDF.transpose()\n",
    "    TotalPriceChangeDFtransposed.columns = ['Price Change']\n",
    "    \n",
    "    return TotalPriceChangeDFtransposed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find which companies performed overall best and worst over the year we will sort the yearly returns of all companies in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "yearlyReturns = yearlyStockReturns(stockPrices)\n",
    "\n",
    "# Sort them\n",
    "yearlyReturnsSorted = yearlyReturns.sort_values(by='Price Change', ascending=False)\n",
    "# Merge on firm data with the symbol as the key (index in both dfs)\n",
    "yearlyReturnsSorted = yearlyReturnsSorted.merge(firms, left_index=True, right_index=True, how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The companies that performed **best** over the year are the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "yearlyReturnsSorted.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pos_top_10 = np.arange(len(yearlyReturnsSorted['Name'][:10])-1,-1,-1)\n",
    "plt.barh(y_pos_top_10, yearlyReturnsSorted['Price Change'][:10]*100,\n",
    "         align='center', color='darkgreen')\n",
    "plt.yticks(y_pos_top_10, yearlyReturnsSorted['Name'][:10])\n",
    "plt.xlabel(\"% change on year\")\n",
    "plt.title(\"Top 10 performing stocks in 2015\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The companies that performed **worst** over the year are the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "yearlyReturnsSorted.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pos_top_10 = np.arange(len(yearlyReturnsSorted['Name'][-10:]))\n",
    "plt.barh(y_pos_top_10, yearlyReturnsSorted['Price Change'][-10:]*100,\n",
    "         align='center', color='darkred')\n",
    "plt.yticks(y_pos_top_10, yearlyReturnsSorted['Name'][-10:])\n",
    "plt.xlabel(\"% change on year\")\n",
    "plt.title(\"Bottom 10 performing stocks in 2015\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Volatility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will try to figure out which companies exhibited **most** and **least** volatility. The volatility of the companies is measured based on the standard deviation of their daily returns over the year. We create the function `volatility` which takes as input a dataframe with the daily returns of the companies and returns a dataframe with the volatility measure of those companies, as defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def volatility(dailyReturns):\n",
    "    ##calculate sds and change into panda dataframe\n",
    "    sdPriceChangeDF = pd.DataFrame(np.std(dailyReturns, axis = 0), \\\n",
    "                                   columns = ['Standard Deviation'])\n",
    "    \n",
    "    return sdPriceChangeDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we sort the dataframe of the volatilities in order to find the most and least volatile companies for 2015."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sdPriceChangeDF = volatility(dailyReturns)\n",
    "\n",
    "# Sort on standard deviation\n",
    "sdPriceChangeDFsorted = sdPriceChangeDF.sort_values(by = 'Standard Deviation', \\\n",
    "                                                    ascending = False)\n",
    "# Merge on firm data with the symbol as the key (index in both dfs)\n",
    "sdPriceChangeDFsortedfull = sdPriceChangeDFsorted.merge(firms, \\\n",
    "                                                        left_index = True, \\\n",
    "                                                        right_index = True, \\\n",
    "                                                        how = 'left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **most volatile** companies for 2015 were the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sdPriceChangeDFsortedfull.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## create list of company ticker names that are in the top 10 most variable\n",
    "columnlist = sdPriceChangeDFsortedfull[0:10].index.values.tolist()\n",
    "##create data frame of price data for just the top 10 most variable companies\n",
    "variablePriceData = stockPrices[columnlist]\n",
    "##scale based on first price\n",
    "variablePricesScaled = variablePriceData.divide(stockPrices[columnlist].ix[0])\n",
    "y_pos_dates = np.arange(len(variablePricesScaled.index))\n",
    "##Set colour scheme\n",
    "colors = ['#9970ab','#5aae61','#4393c3','#de77ae','#35978f','#f768a1','#fec44f','#d0d1e6','#08306b','#a50f15']\n",
    "\n",
    "for i in range(0,len(columnlist)):\n",
    "    plt.plot(y_pos_dates,variablePricesScaled[columnlist[i]], c=colors[i], label=columnlist[i].format(i=i))\n",
    "plt.legend(loc='best')\n",
    "plt.xticks([len(variablePricesScaled.index)/4,\n",
    "            len(variablePricesScaled.index)*2/4,\n",
    "            len(variablePricesScaled.index)*3/4,\n",
    "            len(variablePricesScaled.index)],\n",
    "           [\"Mar-2015\",\"Jun-2015\",\"Sep-2015\",\"Dec-2015\"])\n",
    "plt.legend(loc=5,prop={'size':10})\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **least volatile** companies for 2015 were the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sdPriceChangeDFsortedfull.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## create list of company ticker names that are in the top 10 most variable\n",
    "columnlist = sdPriceChangeDFsortedfull[-10:].index.values.tolist()\n",
    "##create data frame of price data for just the top 10 most variable companies\n",
    "variablePriceData = stockPrices[columnlist]\n",
    "##scale based on first price\n",
    "variablePricesScaled = variablePriceData.divide(stockPrices[columnlist].ix[0])\n",
    "y_pos_dates = np.arange(len(variablePricesScaled.index))\n",
    "##Set colour scheme\n",
    "colors = ['#9970ab','#5aae61','#4393c3','#de77ae','#35978f','#f768a1','#fec44f','#d0d1e6','#08306b','#a50f15']\n",
    "\n",
    "for i in range(0,len(columnlist)):\n",
    "    plt.plot(y_pos_dates,variablePricesScaled[columnlist[i]], c=colors[i], label=columnlist[i].format(i=i))\n",
    "plt.legend(loc='best')\n",
    "plt.xticks([len(variablePricesScaled.index)/4,\n",
    "            len(variablePricesScaled.index)*2/4,\n",
    "            len(variablePricesScaled.index)*3/4,\n",
    "            len(variablePricesScaled.index)],\n",
    "           [\"Mar-2015\",\"Jun-2015\",\"Sep-2015\",\"Dec-2015\"])\n",
    "plt.legend(loc=5,prop={'size':10})\n",
    "axes = plt.gca()\n",
    "axes.set_ylim([0,3])\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the similarities between stock price movements for different companies, we will calculate the correlation between the returns of different stock prices. For two companies with stock price returns $x, y$ and observations for $n$ days, their correlation is given by the following formula:\n",
    "\n",
    "$$r_{xy} = \\frac{n\\sum x_i y_i - \\sum x_i \\sum y_i}{\\sqrt{n\\sum x_i^2 - (\\sum x_i)^2} \\sqrt{n\\sum y_i^2 - (\\sum y_i)^2}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to compute the correlation of the companies, we define function `calCorrelations`, which takes as input a dataframe with the daily returns of the S&P 500 companies and returns their correlations in two different formats:\n",
    "\n",
    "1. A **correlations matrix**, where each element represents the correlation betwwen the two companies indicated by the specific row and column. \n",
    "2. A **graph** where each node represents a company and each edge between two nodes represent the correlation between those two companies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calCorrelations(dailyReturn):\n",
    "    col = dailyReturn.columns\n",
    "    ncol = len(col)\n",
    "    corrMat = np.identity(ncol)\n",
    "    \n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(col.values)\n",
    "        \n",
    "    n = len(dailyReturn)\n",
    "    for i in range(0, ncol):\n",
    "        for j in range(i + 1, ncol):\n",
    "            x = dailyReturn[col[i]]\n",
    "            y = dailyReturn[col[j]]\n",
    "            xsum = sum(x)\n",
    "            ysum = sum(y)\n",
    "            corrMat[i][j] = (n * sum(x * y) - xsum * ysum) / (np.sqrt(n * sum(x**2) - xsum**2) * np.sqrt(n * sum(y**2) - ysum**2))\n",
    "            corrMat[j][i] = corrMat[i][j]\n",
    "            G.add_edge(col[i], col[j], weight = corrMat[i][j])\n",
    "        \n",
    "    return pd.DataFrame(data = corrMat, index = col, columns = col), G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "corMatrix = calCorrelations(stockReturns(stockPrices))[0]\n",
    "corMatrix.iloc[:5, :5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, in order to be able to find and print correlations easily between companies, we provide the following helper functions:\n",
    "\n",
    "1. `cal2CompCor`: It takes as inputs the correlation matrix of the companies, the dataframe with the companies information and the symbols of two companies. It returns their full names and correlation.\n",
    "2. `calTopLow5`: It takes as inputs the correlation matrix of the companies and the symbol of a company. It returns the symbols and correlations of the five highest and lowest correlated companies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cal2CompCor(correlationmatrix, firms, company1, company2):\n",
    "    print(\"The correlation between \" + firms.loc[(company1, company2), :].iloc[0, 0] + \\\n",
    "          \" and \" + firms.loc[(company1, company2), :].iloc[1, 0] + \" is \" + \\\n",
    "          str(corMatrix.loc[company1, company2]) + \".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def highLowCorrelation(correlationmatrix, company):\n",
    "    namesDict = dict()\n",
    "    input_file = csv.DictReader(open('SP_500_firms.csv'))\n",
    "    for row in input_file:\n",
    "        #print(row)\n",
    "        namesDict[row['Symbol']] = [row['Name'],row['Sector']]\n",
    "    \n",
    "    i = correlationmatrix.columns.get_loc(company)  \n",
    "    j = correlationmatrix.ix[ : , i]\n",
    "    high = j.nlargest(6)\n",
    "    highindex = high.index\n",
    "    listofhighcomp = []    \n",
    "    for k in highindex:\n",
    "        listofhighcomp.append(namesDict[k])\n",
    "    low = j.nsmallest(5)\n",
    "    lowindex = low.index\n",
    "    listoflowcomp = []    \n",
    "    for k in lowindex:\n",
    "        listoflowcomp.append(namesDict[k])\n",
    "       \n",
    "    dflistofhighcomp = DataFrame(listofhighcomp)\n",
    "    dfhighindex = DataFrame(highindex)\n",
    "    dflistofhighcomp = dflistofhighcomp.merge(dfhighindex,right_index=True, left_index=True, how = 'left')\n",
    "    dfhigh = DataFrame(high)\n",
    "    table2 = dfhigh.merge(dflistofhighcomp, left_index = True, right_on='0_y', how = 'left')    \n",
    "    table2.index = table2['0_y'].as_matrix()\n",
    "    del table2['0_y']\n",
    "    table2 = table2.rename(columns={company: 'Correlation', '0_x': 'Name of Company', 1: 'Industry'})\n",
    "\n",
    "    dflistoflowcomp = DataFrame(listoflowcomp)\n",
    "    dflowindex = DataFrame(lowindex)\n",
    "    dflistoflowcomp = dflistoflowcomp.merge(dflowindex,right_index=True, left_index=True, how = 'left')\n",
    "    dflow = DataFrame(low)\n",
    "    table = dflow.merge(dflistoflowcomp, left_index = True, right_on='0_y', how = 'left')\n",
    "    table.index = table['0_y'].as_matrix()\n",
    "    del table['0_y']\n",
    "    table = table.rename(columns={company: 'Correlation', '0_x': 'Name of Company', 1: 'Industry'})\n",
    "\n",
    "    return(table2[1:], table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We will use the functions defined above to explore some of the companies in the tech sector, like *Amazon*, *Microsoft*, *Facebook*, *Apple*, and *Google*. All stocks are affected by macroeconomic environments and thus are correlated with each other, either positively or negatively. There are various factors which determine how correlated two stocks are. \n",
    "\n",
    "Usually, stock prices of two companies from the same industry would move in tandem with each other as the market conditions would affect them both in the same way. However, there could also be cases when the market conditions has different effects on companies in the same industry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(highLowCorrelation(corMatrix, 'AMZN')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(highLowCorrelation(corMatrix, 'AMZN')[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the top five stocks correlated with Amazon's stocks, $4$ out of the $5$ companies are IT companies and are growth stocks, which is not surprising. However, Starbucks also happens to have a relatively high correlation with Amazon's stocks though it comes from a completely different industry. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(highLowCorrelation(corMatrix, 'MSFT')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(highLowCorrelation(corMatrix, 'MSFT')[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Microsoft, $2$ out of the top $5$ correlated stocks are from the same industry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(highLowCorrelation(corMatrix, 'FB')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(highLowCorrelation(corMatrix, 'FB')[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Facebook, $4$ out of $5$ of the top correlated companies are in the IT industry. Starbucks also appears in the top 5 for Facebook, surprisingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(highLowCorrelation(corMatrix, 'AAPL')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(highLowCorrelation(corMatrix, 'AAPL')[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Apple, surprisingly its top $3$ correlated stocks are all from the Industrials sector, with the subsequent $2$ being from the IT industry. These results are interesting, as the companies do not appear to have any relation to Apple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(highLowCorrelation(corMatrix, 'GOOG')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(highLowCorrelation(corMatrix, 'GOOG')[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(highLowCorrelation(corMatrix, 'GOOGL')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(highLowCorrelation(corMatrix, 'GOOGL')[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alphabet Inc. is the parent company of Google and has two types of stocks (`GOOG` and `GOOGL`), which are Class A and Class C share respectively. The difference between the two is that the owner of Class A share are allowed to vote, where as the owners of Class C shares are not. \n",
    "The top 5 correlated shares for `GOOG` and `GOOGL` are the same, aside from Microsoft for `GOOG` and Mastercard Inc for `GOOGL`. It is not surprising that the highest stock correlation for `GOOG` is `GOOGL` at 0.98 and vice versa, since they are under the same parent company."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the lowest correlated stocks for each of the aforeentioned tech companies are from companies in the Energy/Industrials/Materials sector, whose stock prices move in conjunction with commodity prices such as oil and gas based on global supply and demand. Thus, it is not surprising that their stock prices have such low correlation with the stock prices of tech companies.\n",
    "\n",
    "For the companies which appear to have no relation to the above tech companies but have correlated stocks, this may be due to chance, as the time period we are looking at is only a year. It would be interesting to see if this correlation is sustained throughout a longer time period, to see if there is really a correlation between the two stocks. This will be investigated further in the in-depth analysis section of the report.\n",
    "\n",
    "Another interesting thing to note is that there appears to be no stocks in the S&P 500 which are strongly negatively correlated with the stocks from above tech companies. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Clustering algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now use the similarity information of the companies based on their correlations to divide them into clusters. The clusters will indicate companies with similar performance over the year 2015 and will be performed using a *greedy* algorithm design which is described as follows:\n",
    "\n",
    "1. Sort the edges in the graph by their weight (ie the correlation)\n",
    "2. Create a single-node set from each node in the graph\n",
    "3. Repeat k times:\n",
    "    1. Pick the highest-weight edge\n",
    "    2. Merge the sets containing the source and the destination of the edge\n",
    "    3. Repeat from A. with the next-highest weight edge\n",
    "    \n",
    "4. Return the remaining sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform the clustering, we define tweo new functions, `sortCorrelations` and `clusteringAlg`, that perform the following tasks:\n",
    "\n",
    "* `sortCorrelations`: Takes as input the correlation matrix of the companies and returns an ordered list of tuples on descending order based on the correlations of the companies. Each tuple has the following format:\n",
    "\n",
    "$$(correlation, company_1, company_2)$$\n",
    "\n",
    "* `clusteringAlg`: takes as input the list of tuple created by the previous function and a constant $k$, which represents the iterations of the algorithm, and clusters the companies based on the algorithm described previously. The output of the function is a list of sets, where each set represents a cluster of companies and a list of integers where each integer represents the number of clusters at each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sortCorrelations(corMatrix):\n",
    "    n = int(corMatrix.shape[0])\n",
    "    corList = []\n",
    "    for i in range(1, n):\n",
    "        for j in range(0, i):\n",
    "            corList.append((corMatrix.iloc[i, j], corMatrix.columns.values[i], \\\n",
    "                            corMatrix.columns.values[j]))\n",
    "    return sorted(corList, reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clusteringAlg(corList, k = 0):\n",
    "    \"\"\"\n",
    "    Input:  \n",
    "    corList: The ordered list of tuples which include the\n",
    "              correlations between firms and the firms themselves\n",
    "    k: The number of iterations for the clustering algorithm\n",
    "    Output: \n",
    "    A list of sets where each set represents an individual \n",
    "    cluster\n",
    "    \"\"\"\n",
    "    # Initialize the list of sets. Each set represents a cluster\n",
    "    # which initialy includes only one firm\n",
    "    sets = []\n",
    "    noOfClusters = []\n",
    "    for i in range(len(corList)):\n",
    "        if not({corList[i][1]} in sets):\n",
    "            sets.append({corList[i][1]})\n",
    "        if not({corList[i][2]} in sets):\n",
    "            sets.append({corList[i][2]}) \n",
    "    inNoOfClusters = len(sets)\n",
    "    # Repeat the algorithm k times\n",
    "    # In each iteration we check the k-th tuple of correlations list\n",
    "    # and whether the 2 firms in that tuple are already in the same\n",
    "    # set. If they do, we move on to the next tuple, otherwise we merge\n",
    "    for j in range(min(k, len(corList))):\n",
    "        nd1 = corList[j][1]\n",
    "        nd2 = corList[j][2]\n",
    "        fl1, fl2 = False, False \n",
    "        noOfClusters.append(inNoOfClusters - len(sets))\n",
    "        for i in range(len(sets)):\n",
    "            if (nd1 in sets[i]) and fl1 == False:\n",
    "                idx1 = i\n",
    "                fl1 = True\n",
    "            if (nd2 in sets[i]) and fl2 == False:\n",
    "                idx2 = i\n",
    "                fl2 = True\n",
    "        if idx1 != idx2:\n",
    "            sets[idx1] = sets[idx1].union(sets[idx2])\n",
    "            sets.remove(sets[idx2])\n",
    "    # Return the final list of sets\n",
    "    return sets, noOfClusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corList = sortCorrelations(corMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clusters, noClusters = clusteringAlg(corList, k = 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following diagram we observe how the number of clusters increases as the number of iterations increases from $1$ to $10,000$. We define clusters are groups of companies that include more than one company. We observe that the slope of the line is more steep during the first iterations but as the number of iterations increases, the number of clusters increases at a slower pace. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t = np.arange(1, 10001)\n",
    "plt.plot(t, noClusters, '-')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Number of iterations')\n",
    "plt.title('Clusters versus iterations')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The clustering algorithm implemented above is called **single-linkage clustering** and is one of the many methods of **hierarchical clustering**. Single-linkage clustering is based on agglomerative clustering. It involves starting with each firm in a cluster of its own and then combining two clusters at each iteration of the algorithm. It chooses the two clusters that are 'close to' i.e. highly correlated with each other.\n",
    "\n",
    "This method is also known as **nearest neighbour clustering** which is used in the traveling salesman problem (TSP). In TSP, a salesman wants to travel to $N$ number of cities that are connected to each other. The connections between cities have weights that represent the cost or distance. The salesman wants to minimize the total cost incurred or total distance travelled, i.e. the salesman is looking for cities that are 'close to' each other in the same way we look for stock prices which are 'close to' each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to evaluate our clustering algorithm, we will focus on the resulting clusters for different values of $k$ and see if the companies included in those have similarities between them and whether their stock prices and daily returns perform in a similar way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For that purpose we define the function `companyTracker` which take as input the correlation list previously computed, the symbol of a company, the dataframe which contains the companies information and three integers, which specify the different values of $k$ for which we will execute the clustering algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def companyTracker(corList, company, firms, ks, kf, kint):\n",
    "    kValues = np.arange(ks, kf, kint)\n",
    "    sets = []\n",
    "    for k in kValues:\n",
    "        clusters = clusteringAlg(corList, k)[0]\n",
    "        print('k = ' + str(k))\n",
    "        for i in range(len(clusters)):\n",
    "            if company in clusters[i]:\n",
    "                if len(clusters[i]) == 1:\n",
    "                    display(firms.loc[ticker, :])\n",
    "                else:\n",
    "                    display(firms.loc[clusters[i], :].sort_values(['Sector', 'Name']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will see how the aforementioned companies are clustered when the number of iterations $k = \\{50, 100, 150\\}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "companyTracker(corList, 'BAC', firms, 30, 60, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "companyTracker(corList, 'BAC', firms, 50, 100, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "companyTracker(corList, 'BAC', firms, 100, 200, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "companyTracker(corList, 'BAC', firms, 500, 1000, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "companyTracker(corList, 'BAC', firms, 1000, 2000, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the clustering algorithm, for increasing values of $k$, it is observed that as $k$ increases more elements are added to the existing clusters and more new clusters are formed. Clusters are formed with stocks belonging to firms within the same industry. However, after a certain value of $k$, we see clusters merging and becoming more 'broad' – for example, what was initially two clusters – insurance and banking – becomes a general 'finance' cluster. \n",
    "\n",
    "In the above tables, *Bank of America Merrill Lynch* was tracked for different values of $k$ to see how the composition of its cluster changes when $k=\\{30, 50, 100, 500, 1000\\}$. For values of $k$ equal or less than $500$, we can only see companies from the Financial sector within its cluster. For $k = 1000$, firms from the Consumer Discretionary, Industrial and Information Technology sector join the sector. This may happen because one or two stocks from these indsutries may have a lower correlation with companies from the Financial sector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define the function `plotSetPrices` which takes as inputs the stock prices of the S&P 500 companies, the resulting clusters of the clustering algorithm and the symbol of a company. TWhat the function does is to plot the normalised prices of all the the companies that belong to the cluster of the specified company."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plotSetPrices(stockPrices, clusters, company):\n",
    "    \n",
    "    for i in range(len(clusters)):\n",
    "            if company in clusters[i]:\n",
    "                normPrices = stockPrices.loc[:, clusters[i]]\n",
    "                normPrices = normPrices / normPrices.iloc[0, :]\n",
    "                normPrices.plot(legend = True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clusters, noClusters = clusteringAlg(corList, k = 30)\n",
    "plotSetPrices(stockPrices, clusters, 'BAC')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The extra part "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In-depth analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Stock correlations of the 5 tech stocks in 2014"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the main section of the report, we looked at the top correlated stocks with the 5 tech companies in 2015. We will now look to see whether these correlations exist in 2014 as well. Stock prices of the S&P500 companies in 2014 were drawn from the Yahoo Finance website and correlations are calculated based on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pull stock tickers from S&P500 list and convert them into a list\n",
    "\n",
    "stocktickers = corMatrix.columns\n",
    "stocktickers = list(stocktickers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To pull data from the Yahoo Finance website, we used helper functions from hw2.py to generate the helper function `getStockfromYahoo`, which requires two inputs: the year of the data and the list of stock tickers of interest. It returns a dataframe of stock prices over the year for the specified stocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas_datareader.data as web\n",
    "from datetime import datetime\n",
    "\n",
    "#additional helper functions from hw2.py file\n",
    "\n",
    "def getStock(symbol, start, end):\n",
    "    \"\"\"\n",
    "    Downloads stock price data from Yahoo Finance\n",
    "    Returns a pandas dataframe.\n",
    "    \"\"\"\n",
    "    df =  web.DataReader(symbol, 'yahoo', start, end)\n",
    "    return df\n",
    "\n",
    "def getClose(df):\n",
    "    \"\"\"\n",
    "    Returns stock price dataframe's adjusted closing price as a list\n",
    "    \"\"\"\n",
    "    L = df['Adj Close'].values.tolist()\n",
    "    return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getStockfromYahoo(year, stocktickers):\n",
    "    start = datetime(year,1,1)\n",
    "    finish = datetime(year,12,31)\n",
    "    stockdf = DataFrame()\n",
    "    a = 0\n",
    "    \n",
    "    for i in stocktickers:\n",
    "        try:\n",
    "            if a == 0:\n",
    "                stock = getStock(i,start, finish)\n",
    "                stock = stock.loc[:, 'Close']\n",
    "                stockdf = DataFrame(stock)\n",
    "                stockdf = stockdf.rename(columns={'Close': i})\n",
    "                a = a + 1\n",
    "            else:            \n",
    "                stock = getStock(i,start, finish)\n",
    "                stock = stock.loc[:, 'Close']\n",
    "                stockdf = stockdf.join(stock)\n",
    "                stockdf = stockdf.rename(columns={'Close': i})\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    return stockdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#create dataframe with stock prices\n",
    "\n",
    "data_2014 = getStockfromYahoo(2014, stocktickers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#calculate stock returns for 2014 data\n",
    "\n",
    "sr_2014 = stockReturns(data_2014)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create correlation matrix for 2014\n",
    "\n",
    "corr_2014 = calCorrelations(sr_2014)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'highlowCorrelation' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-249f49d7e54c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#top and bottom correlated companies for FB\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mhighlowCorrelation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorr_2014\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'FB'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'highlowCorrelation' is not defined"
     ]
    }
   ],
   "source": [
    "#top and bottom correlated companies for FB\n",
    "\n",
    "pd.DataFrame(highLowCorrelation(corr_2014, 'FB')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(highLowCorrelation(corr_2014, 'FB')[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#correlation between BRK-B and MSFT\n",
    "\n",
    "cal2CompCor(corr_2014, firms, 'BRK-B', 'MSFT')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we looked at the top and bottom correlated stocks for the 5 tech companies in 2014, we saw a change in the results. For example, for Facebook in 2015, the top correlated companies were mostly from the IT industry, but in 2014, it was most highly correlated with companies from the healthcare industry.\n",
    "\n",
    "Overall, most of the companies which had the highest correlation with the 5 tech companies in 2015 did not appear in the top 5 correlated companies in 2014, which suggests that many of the surprising results seen before e.g. Starbucks stocks having high correlation with Facebook stocks in 2015, were probably due to chance. (Starbucks stocks and Facebook stocks have a correlation of 0.60 and 0.34 in 2015 and 2014 respectively)\n",
    "\n",
    "On the other hand, for Microsoft and Berkshire-Hathaway, we still find positive correlation between the two stocks in 2014, though not as strong (0.59 in 2015, 0.43 in 2014). This is interesting because though the two companies are from different industries, their stocks are moderately correlated over a period of two years. This may warrant further investigation, to see whether this relationship exists over an even longer period of time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring other clustering methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Any notes or comments about the report to be listed here:*\n",
    "\n",
    "* "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
